{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Late Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\el debs\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from models.loader import load_model\n",
    "from data.dataset import get_flair_loader, get_seg_loader, get_t2_loader, get_t1ce_loader, get_t1_loader\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn   \n",
    "import torchvision.transforms as T\n",
    "from torchsummary import summary\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import ndcg_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "number_patient = 200\n",
    "\n",
    "# Models class\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, latent_dim=256):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.BatchNorm2d(32, momentum=0.9),\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.BatchNorm2d(64, momentum=0.9),\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.BatchNorm2d(128, momentum=0.9),\n",
    "            nn.Conv2d(128, 256, 3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.BatchNorm2d(256, momentum=0.9),\n",
    "            nn.Conv2d(256, 256, 3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.BatchNorm2d(256, momentum=0.9),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 3 * 4, self.latent_dim)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.latent_dim, 256 * 3 * 4),\n",
    "            nn.Unflatten(1, (256, 3, 4)),\n",
    "            nn.ConvTranspose2d(self.latent_dim, 256, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.BatchNorm2d(256, momentum=0.9),\n",
    "            nn.ConvTranspose2d(256, 128, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.BatchNorm2d(128, momentum=0.9),\n",
    "            nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.BatchNorm2d(64, momentum=0.9),\n",
    "            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.BatchNorm2d(32, momentum=0.9),\n",
    "            nn.ConvTranspose2d(32, 1, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))\n",
    "\n",
    "    def train_reconstruction(self, loader, epochs=10, lr=0.001):\n",
    "        self.to(device)\n",
    "        self.train()\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        criterion = nn.MSELoss()\n",
    "        for epoch in range(epochs):\n",
    "            for i, x in enumerate(loader):\n",
    "                x = x.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                x_reconstructed = self.forward(x)\n",
    "                loss = criterion(x_reconstructed, x)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                if i % 100 == 0:\n",
    "                    print(f\"Epoch {epoch}, batch {i}/{len(loader)}, loss {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1050/1050 [00:02<00:00, 417.83it/s]\n",
      "100%|██████████| 1050/1050 [00:03<00:00, 349.77it/s]\n",
      "100%|██████████| 1050/1050 [00:02<00:00, 352.70it/s]\n",
      "100%|██████████| 1050/1050 [00:03<00:00, 342.80it/s]\n",
      "100%|██████████| 1050/1050 [00:03<00:00, 336.97it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 435.72it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 165.92it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 333.89it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 331.68it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 350.88it/s]\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    \"seg\" : load_model(\"./models/seg.pth\"),\n",
    "    \"t1\" : load_model(\"./models/t1.pth\"),\n",
    "    \"t1ce\" : load_model(\"./models/t1ce.pth\"),\n",
    "    \"flair\" : load_model(\"./models/flair.pth\"),\n",
    "    \"t2\" : load_model(\"./models/t2.pth\")\n",
    "}\n",
    "# All the dataset for each model\n",
    "data = {\n",
    "    \"seg\" : get_seg_loader(),\n",
    "    \"t1\" : get_t1_loader(),\n",
    "    \"t1ce\" : get_t1ce_loader(),\n",
    "    \"flair\" : get_flair_loader(),\n",
    "    \"t2\" : get_t2_loader()\n",
    "}\n",
    "\n",
    "matrix = {}\n",
    "similarities = {}\n",
    "candidates = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_matrix_for (model_key):\n",
    "    \"\"\"\n",
    "    Return features matrix for each patient \n",
    "    \"\"\"\n",
    "\n",
    "    result = None\n",
    "    arrays = []\n",
    "\n",
    "    for index, batch in enumerate (data[model_key]) :\n",
    "        arrays.append(models[model_key].encode(batch))\n",
    "\n",
    "    result = torch.cat(tuple(arrays), dim=0)\n",
    "\n",
    "    return result\n",
    "\n",
    "def get_similarity_matrix_for (model_key) :\n",
    "    \"\"\"\n",
    "    Return a similarity matrix for a specific model\n",
    "    \"\"\"\n",
    "\n",
    "    distance_matrix = np.zeros((number_patient, number_patient))\n",
    "    result = get_features_matrix_for(model_key)\n",
    "\n",
    "    for patient_1 in range (0, number_patient) :\n",
    "        for patient_2 in range (0, number_patient) :\n",
    "            distance_matrix[patient_1, patient_2] = torch.linalg.norm(result[patient_1] - result[patient_2], ord=2)\n",
    "        \n",
    "    return distance_matrix\n",
    "\n",
    "def get_most_similar_patient (key_model, similarity_matrix) :\n",
    "    \"\"\"\n",
    "    Sort patient index in order of most relevent to less relevent\n",
    "    \"\"\"\n",
    "\n",
    "    temp = np.zeros((number_patient, number_patient))\n",
    "\n",
    "    for candidate in range (number_patient) :\n",
    "        temp[candidate] = np.argsort(similarity_matrix[key_model][candidate])\n",
    "\n",
    "    return temp\n",
    "\n",
    "def get_sort_similar (key_model, similarity_matrix) :\n",
    "    \"\"\"\n",
    "    Sort patient values\n",
    "    \"\"\"\n",
    "\n",
    "    temp = np.zeros((number_patient, number_patient))\n",
    "\n",
    "    for candidate in range (number_patient) :\n",
    "        temp[candidate] = np.sort(similarity_matrix[key_model][candidate])\n",
    "\n",
    "    return temp\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting most similar patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in data :\n",
    "    matrix[key] = get_features_matrix_for(key)\n",
    "    similarities[key]  = get_similarity_matrix_for (key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def borda_count (modals_keys) :\n",
    "    \"\"\"\n",
    "    Utilize borda_count to determine the candidates for \n",
    "    \"\"\"\n",
    "\n",
    "    temp_sim_indice = {}\n",
    "    temp_sim_sort = {}\n",
    "\n",
    "    end_most_relevant_candidate = np.zeros((number_patient, number_patient))\n",
    "    end_most_relevant_candidate_values = np.zeros((number_patient, number_patient))\n",
    "\n",
    "    for key in modals_keys :\n",
    "\n",
    "        temp_sim_indice [key] = get_most_similar_patient (key, similarities)\n",
    "        temp_sim_sort [key] = get_sort_similar (key, similarities)\n",
    "\n",
    "\n",
    "    # Getting the N ith patient in all models\n",
    "    for patient in range (number_patient) :\n",
    "\n",
    "        temp = {}\n",
    "        candidates_temp = {}\n",
    "\n",
    "        for key in modals_keys :\n",
    "            temp [key] = temp_sim_indice [key][patient]\n",
    "\n",
    "        for key in modals_keys :\n",
    "\n",
    "            # Counting ponderation\n",
    "\n",
    "            for i, candidate in enumerate( temp[key]) :\n",
    "                if candidate not in candidates_temp :\n",
    "                    candidates_temp[candidate] = number_patient - i\n",
    "                else :\n",
    "                    candidates_temp[candidate] += number_patient - i \n",
    "\n",
    "        sorted_list_patient = sorted (candidates_temp, key=candidates_temp.get)\n",
    "        sorted_list_patient.reverse()\n",
    "\n",
    "        value_sorted = sorted (candidates_temp.values())\n",
    "        value_sorted.reverse()\n",
    "\n",
    "        end_most_relevant_candidate[patient] = np.array(sorted_list_patient)\n",
    "        end_most_relevant_candidate_values[patient] = np.array(value_sorted)\n",
    "        # Borda count vote for the five models\n",
    "\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"candidates\" : end_most_relevant_candidate,\n",
    "        \"values_candidates\" : end_most_relevant_candidate_values\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_matrix = borda_count (data.keys())\n",
    "\n",
    "candidates_matrix['candidates'] = candidates_matrix['candidates'][:,1:]\n",
    "candidates_matrix['values_candidates']= candidates_matrix['values_candidates'][:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate NDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.82245911, 0.8577357 , 0.86001779, 0.84384805, 0.8239485 ,\n",
       "        0.85025024, 0.84933212, 0.84582292, 0.82992151, 0.83515181,\n",
       "        0.84552729, 0.82532444, 0.82505279, 0.8401196 , 0.83646798,\n",
       "        0.8505333 , 0.86071333, 0.83575711, 0.83674904, 0.83906377,\n",
       "        0.84711791, 0.84435709, 0.82772311, 0.826835  , 0.86342462,\n",
       "        0.84718996, 0.84024042, 0.82002108, 0.82232115, 0.84775996,\n",
       "        0.82704775, 0.82707302, 0.85849851, 0.86810165, 0.84364039,\n",
       "        0.86066422, 0.85222457, 0.83879503, 0.84431929, 0.84270972,\n",
       "        0.83772172, 0.85594114, 0.83910083, 0.83538075, 0.85595874,\n",
       "        0.83633646, 0.84436767, 0.8666647 , 0.82471694, 0.8540424 ,\n",
       "        0.84227408, 0.8439962 , 0.86007306, 0.86854879, 0.83515426,\n",
       "        0.8585878 , 0.82614278, 0.83060612, 0.864261  , 0.86138798,\n",
       "        0.84850773, 0.84259706, 0.83970414, 0.87613908, 0.86706485,\n",
       "        0.85716744, 0.83735291, 0.84925946, 0.83133931, 0.85560747,\n",
       "        0.85127549, 0.8539983 , 0.85831997, 0.86167992, 0.85752657,\n",
       "        0.83474651, 0.85899738, 0.88009011, 0.87585039, 0.84971218,\n",
       "        0.8409753 , 0.86391096, 0.86146886, 0.86504977, 0.87273383,\n",
       "        0.84924995, 0.8578671 , 0.83536276, 0.85491326, 0.84795666,\n",
       "        0.86632731, 0.86156502, 0.86459129, 0.87397335, 0.88778101,\n",
       "        0.8564613 , 0.84977129, 0.87061909, 0.86469127, 0.87442849,\n",
       "        0.85882072, 0.87376057, 0.8524646 , 0.87953158, 0.85240203,\n",
       "        0.86342824, 0.86178892, 0.8627068 , 0.85060012, 0.8536293 ,\n",
       "        0.84354971, 0.85515703, 0.87971587, 0.88165256, 0.83467947,\n",
       "        0.86495959, 0.86908627, 0.87150396, 0.87430036, 0.86157697,\n",
       "        0.83183156, 0.87485495, 0.87054975, 0.85398335, 0.85720391,\n",
       "        0.87482256, 0.85854999, 0.87944074, 0.86984624, 0.84666704,\n",
       "        0.87404322, 0.8537826 , 0.89205918, 0.86537633, 0.87377605,\n",
       "        0.85096034, 0.83896371, 0.86136454, 0.87336023, 0.87878424,\n",
       "        0.85981308, 0.85349612, 0.87382994, 0.87530574, 0.8747373 ,\n",
       "        0.89400739, 0.89120567, 0.88765644, 0.89659029, 0.88722352,\n",
       "        0.88930132, 0.85893865, 0.88641836, 0.89500313, 0.86783791,\n",
       "        0.88797502, 0.87210285, 0.86852058, 0.8701    , 0.8877282 ,\n",
       "        0.85049633, 0.89414695, 0.88236982, 0.87498322, 0.87960291,\n",
       "        0.89162325, 0.8934462 , 0.8716979 , 0.86808823, 0.86441344,\n",
       "        0.89181158, 0.89674403, 0.87726217, 0.88183969, 0.87336915,\n",
       "        0.8883839 , 0.89766054, 0.8765977 , 0.89537516, 0.86815025,\n",
       "        0.89558951, 0.86817942, 0.90301902, 0.86169037, 0.88232505,\n",
       "        0.89092837, 0.88789716, 0.89285325, 0.87097951, 0.86751838,\n",
       "        0.86163095, 0.87522469, 0.89755876, 0.88856472, 0.87086584,\n",
       "        0.88622374, 0.88072665, 0.86212178, 0.8525077 , 0.86870151]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ground truth\n",
    "with open ('./data/IoU.pickle', \"rb\") as file :\n",
    "    djakkar =  pickle.load(file)\n",
    "\n",
    "djakkar_index = djakkar\n",
    "\n",
    "# Patient croissant matrix\n",
    "most_similar_patient_matrix = np.zeros(djakkar_index.shape)\n",
    "\n",
    "for x in range (most_similar_patient_matrix.shape[0]) :\n",
    "    most_similar_patient_matrix[x] = np.argsort(djakkar_index[x])\n",
    "\n",
    "\n",
    "most_similar_patient_matrix\n",
    "\n",
    "sorted_djakkar_index = np.zeros(djakkar_index.shape)\n",
    "\n",
    "for x in range (sorted_djakkar_index.shape[0]) :\n",
    "    sorted_djakkar_index[x] = np.sort(djakkar_index[x])\n",
    "\n",
    "\n",
    "sorted_djakkar_index\n",
    "\n",
    "testing_size = djakkar_index.shape[0]\n",
    "\n",
    "# Getting the NDCG score\n",
    "ndcg_matrix = np.zeros((1, testing_size))\n",
    "\n",
    "for x in range (testing_size) :\n",
    "    ndcg_matrix [0, x] = ndcg_score(np.asarray([most_similar_patient_matrix[:,1:][x]]), np.asarray([sorted_djakkar_index[:,1:][x]]))\n",
    "\n",
    "ndcg_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate models ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 1652.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.86890092 0.86077733 0.84490902 0.86514623 0.89320719 0.86014605\n",
      "  0.88793426 0.86158562 0.86527133 0.8343647  0.84706034 0.83822318\n",
      "  0.84880401 0.87901092 0.83864675 0.88387067 0.8602223  0.85832263\n",
      "  0.84667824 0.85879981 0.85675056 0.86091919 0.83034227 0.87970573\n",
      "  0.85613518 0.86556851 0.87039555 0.86008233 0.85723729 0.85246656\n",
      "  0.85202106 0.85675549 0.85334103 0.87967986 0.86045141 0.84151582\n",
      "  0.88475922 0.85939146 0.88097416 0.86961371 0.88414596 0.85337048\n",
      "  0.83970182 0.8377998  0.82883323 0.90470003 0.8383206  0.8464024\n",
      "  0.89513772 0.84446946 0.87141245 0.8781591  0.86777451 0.8428604\n",
      "  0.86948239 0.87044505 0.85731634 0.84926237 0.85921801 0.84333048\n",
      "  0.86830662 0.84163462 0.88417557 0.88307683 0.84150225 0.8450156\n",
      "  0.88040025 0.84613302 0.86064525 0.84911595 0.8531183  0.85279538\n",
      "  0.86061261 0.87583728 0.84010454 0.8525269  0.88384196 0.85012938\n",
      "  0.87670923 0.83459462 0.86644855 0.85435139 0.83589938 0.83550671\n",
      "  0.84743478 0.84835012 0.85957804 0.88257221 0.88248805 0.87384847\n",
      "  0.85090298 0.86204462 0.8680398  0.86954924 0.8671438  0.84569953\n",
      "  0.86182463 0.84828398 0.84616377 0.85411429 0.857306   0.87111568\n",
      "  0.84970845 0.86687434 0.86870443 0.87719355 0.85621747 0.87575877\n",
      "  0.85262042 0.86584009 0.86502516 0.87623292 0.87021398 0.86464062\n",
      "  0.82760754 0.87313177 0.89087766 0.84092867 0.87416586 0.8591271\n",
      "  0.8575244  0.84078536 0.82615802 0.86006543 0.88352532 0.89052844\n",
      "  0.83332592 0.85792515 0.84292234 0.89196305 0.87705109 0.85770891\n",
      "  0.84313507 0.8686263  0.84317347 0.85925814 0.84030309 0.8605447\n",
      "  0.86370017 0.88861052 0.86827753 0.84703539 0.84782306 0.88947861\n",
      "  0.88576213 0.87256193 0.83927452 0.85067419 0.84985526 0.87987728\n",
      "  0.83601517 0.86960076 0.85916977 0.85632025 0.86337517 0.87642607\n",
      "  0.85503931 0.85029893 0.85329513 0.85883435 0.84072129 0.86593565\n",
      "  0.85667657 0.86826531 0.88701638 0.84611084 0.8509322  0.87139964\n",
      "  0.85195871 0.86244226 0.87769565 0.86576694 0.86076206 0.8622286\n",
      "  0.85291126 0.86806081 0.83249847 0.90020783 0.8444504  0.85379048\n",
      "  0.86563486 0.8647662  0.87079228 0.88680963 0.85329656 0.86599264\n",
      "  0.8647129  0.85320988 0.8302314  0.84955558 0.85860648 0.85617902\n",
      "  0.86284814 0.87458391 0.84115947 0.83663372 0.87951736 0.86419809\n",
      "  0.84831671 0.89241401]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ndcg_model_scores = np.zeros( (1, number_patient) )\n",
    "\n",
    "for patient in tqdm (range (number_patient)) :\n",
    "    ndcg_model_scores [0, patient] = ndcg_score([candidates_matrix[\"candidates\"][patient]], [candidates_matrix[\"values_candidates\"][patient]])\n",
    "\n",
    "print (ndcg_model_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"./data/late_fusion_ndcg.pickle\", \"wb\") as file:\n",
    "    pickle.dump(ndcg_model_scores, file)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a4c1b5ee30376b71eaa339265b8dea39d4cdab92fa92b725320d3eb2e4642e28"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
